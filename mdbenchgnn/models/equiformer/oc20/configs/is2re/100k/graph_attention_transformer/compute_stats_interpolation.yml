includes:
- oc20/configs/is2re/100k/base.yml


compute_stats: True


model:
  name: graph_attention_transformer
  irreps_node_embedding: '256x0e+128x1e'
  num_layers: 8
  irreps_node_attr: '1x0e'
  use_node_attr: False
  irreps_sh: '1x0e+1x1e'
  max_radius: 6.0
  number_of_basis: 128
  fc_neurons: [64, 64] 
  use_atom_edge_attr: False
  irreps_atom_edge_attr: '1x0e'
  irreps_feature: '512x0e'
  irreps_head: '32x0e+16x1e'
  num_heads: 8
  irreps_pre_attn: '256x0e+128x1e'
  rescale_degree: False
  nonlinear_message: False
  irreps_mlp_mid: '768x0e+384x1e'
  norm_layer: 'layer'
  alpha_drop: 0.2
  proj_drop: 0.0
  out_drop: 0.0 
  drop_path_rate: 0.0
  otf_graph: False
  use_pbc: True
  max_neighbors: 50
  
  
optim:
  batch_size: 16
  eval_batch_size: 16
  num_workers: 4
  lr_initial: 1.e-4
  max_epochs: 20
  optimizer: AdamW
  optimizer_params:
    weight_decay: 0.01
  scheduler: LambdaLR
  scheduler_params:
    lambda_type: cosine
    warmup_factor: 0.2
    warmup_epochs: 2
    lr_min_factor: 1.e-2
    
  use_interpolate_init_relaxed_pos: True